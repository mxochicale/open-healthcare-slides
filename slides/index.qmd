---
title: ""
subtitle: ""

format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    controls: false
    width: 1280 #1440 #1920 #1050
    height: 720
    margin: 0.025
    controls-layout: bottom-right
    preview-links: auto
    logo: favicon.svg
    theme: dark
    #css:
      #- css/default.css
      #- css/callouts-html.css
    #footer: <https://quarto.org>
  gfm:
    author: Miguel Xochicale
#revealjs-plugins:
#    - subtitles
---

# {.title-slide .centeredslide background-iframe="https://saforem2.github.io/grid-worms-animation/" loading="lazy"}

::: {style="background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"}
<span style="color:#939393; font-size:1.5em; font-weight: bold;">Can Open-Source Software</span>  
<span style="color:#939393; font-size:1.5em; font-weight: bold;">Revolutionise Healthcare?</span>  

[<br>&nbsp;]{style="padding-bottom: 0.5rem;"}  
[{{< fa solid home >}}](http://mxochicale.github.io/) Miguel Xochicale, PhD     
[[[{{< fa brands github >}} `mxochicale/`](https://github.com/mxochicale/)]{style="border-bottom: 0.5px solid #00ccff;"}[[`open-healthcare-slides`](https://github.com/mxochicale/open-healthcare-slides)]{style="border-bottom: 0.5px solid #00ccff;"}]{style="font-size:0.8em;"}
:::

::: footer
[2024-04-20 @ [Link for grid-worms-animation 2023](https://github.com/saforem2/grid-worms-animation/)]{.dim-text style="text-align:left;'}
:::


# Overview {style="font-size: 90%;"} 

* [My trajectory](#sec-mt)
* Software as a Medical Device (SaMD)
* Uses cases
  * [Fetal Ultrasound Image Synthesis](#sec-fus)
  * Real-time AI diagnosis: 
    * Endoscopy-based video 
    * eye movement disorders
* Takeaways

## My trajectory

::: {#sec-mt style="margin-top: 0px; font-size: 50%;"} 
![](figures/mx.svg){fig-align="center" width="100%"}
:::


# Software as a Medical Device (SaMD)

## Regulation vs innovation in MedTech

::: { style="margin-top: 0px; font-size: 50%;"} 
![](figures/regulation-innovation/balance-between-regulation-innovation.svg){fig-align="center" width="100%"}
:::

## IEC 62304 standard for  software

::: { style="margin-top: 0px; font-size: 50%;"} 
![](figures/regulation-innovation/iec62304.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
https://www.iso.org/standard/38421.html
:::


## Good ML practices by FDA

::: { style="margin-top: 0px; font-size: 50%;"} 
![](figures/regulation-innovation/goodALMLFDA.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
US-FDA-Artificial-Intelligence-and-Machine-Learning-Discussion-Paper 
:::

::: {.notes}
Regulatory Framework for Modifications to
(AI/ML)-Based Software as a Medical Device (SaMD)
:::


## FDA-approved AI-based Medical Devices

::: { style="margin-top: 0px; font-size: 50%;"} 
![](figures/regulation-innovation/fda-approved-ai-based-med-devs.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
Benjamens, S., Dhunnoo, P. and Meskó, B. The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online
database. npj Digit. Med. 3, 118 (2020). 
:::





# Fetal Ultrasound Image Synthesis

## Dating Ultrasound Scan (12 week scan)

::: {#sec-fus style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/12-week-scan.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
Wright-Gilbertson M. 2014 in PhD thesis; https://en.wikipedia.org/wiki/Gestational_age; National-Health-Service 2021. Screening for down’s syndrome,
edwards’ syndrome and patau’s syndrome. https://www.nhs.uk/pregnancy/your- pregnancy- care
:::


## Challenges of US biometric measurements

* Operator dependant,
* Clinical system dependant,
* Fetal position,
* Similar morphological and echogenic characteristics in the US,
* Few public datasets are available (we have only found two)

::: {style="font-size: 40%;"}
Sciortino et al. in Computers in Biology and Medicine 2017 https://doi.org/10.1016/j.compbiomed.2017.01.008; He et al. in Front. Med. 2021
https://doi.org/10.3389/fmed.2021.729978
:::


## TransThalamic
Fetal Brain Ultrasound Image Dataset

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/fetal-planes-dataset-TransThalami.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
Burgos-Artizzu, X et al. (2020). FETAL PLANES DB: Common maternal-fetal ultrasound images [Data set]. In Nature Scientific Reports (1.0,
Vol. 10, p. 10200). Zenodo. https://doi.org/10.5281/zenodo.3904280
:::

## TransCerebellum Plain
Fetal Brain Ultrasound Image Dataset

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/fetal-planes-dataset-TransCerebellum.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
Burgos-Artizzu, X et al. (2020). FETAL PLANES DB: Common maternal-fetal ultrasound images [Data set]. In Nature Scientific Reports (1.0,
Vol. 10, p. 10200). Zenodo. https://doi.org/10.5281/zenodo.3904280
:::

## TransVentricular Plane
Fetal Brain Ultrasound Image Dataset

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/fetal-planes-dataset-TransVentricular.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
Burgos-Artizzu, X et al. (2020). FETAL PLANES DB: Common maternal-fetal ultrasound images [Data set]. In Nature Scientific Reports (1.0,
Vol. 10, p. 10200). Zenodo. https://doi.org/10.5281/zenodo.3904280
:::

## Research Questions 

* Research and implement deep learning methods for generating synthetic fetal ultrasound images for both normal and abnormal cases, 
* Propose and apply methods to evaluate quantitative and qualitative images of fetal us image synthesis.

## GAN-based fetal imaging

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/gan-based-fetal-imaging.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
(a) Bautista et al. 2022, ”Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs” MIUA
https://github.com/budai4medtech/miua2022 (b) Liu et al. 2021 ”Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image
Synthesis” https://arxiv.org/abs/2101.04775
:::


## AI/ML pipeline
::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/ai-pipeline-gans-based-pipeline-fetal-imaging.svg){fig-align="center" width="100%"}
:::


## Image Quality Assessment

Quaility of synthesised images are evaluated with Frechet inception distance (FID), measuring the distance 
between distributions of synthetised and original images (Heusel et al., 2017).

The lower the FID number is, the more similar the synthetised images are to the original ones. 
FID metric showed to work well with fetal head US compared to other metrics (Bautista et al., 2012).

::: {style="font-size: 40%;"}
M. Iskandar et al. "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis" in MIDL2023. https://github.com/budai4medtech/midl2023
:::


## Methods

Diffusion-Super-Resolution-GAN (DSR-GAN)
Transformer-based-GAN

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/methods-diffusion-Super-Resolution-GAN-transformer-based-GAN.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
M. Iskandar et al. "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis" in MIDL2023. https://github.com/budai4medtech/midl2023
:::


## Experiments: Design and results

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/results-only-images.svg){fig-align="center" width="100%"}
:::


::: {style="font-size: 40%;"}
M. Iskandar et al. "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis" in MIDL2023. https://github.com/budai4medtech/midl2023
:::



## Experiments: Design and results

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/results-plots.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
M. Iskandar et al. "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis" in MIDL2023. https://github.com/budai4medtech/midl2023
:::


## {{< fa brands github >}} github.com/budai4medtech/midl2023

::: {#sec-hp style="margin-top: 0px; font-size: 10%;"} 
![](figures/fetal-ultrasound-image-synthesis/github-repository.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
M. Iskandar et al. "Towards Realistic Ultrasound Fetal Brain Imaging Synthesis" in MIDL2023. https://github.com/budai4medtech/midl2023
:::


## Fetal US imaging with Diffusion models

::: {#sec-hp style="margin-top: 0px; font-size: 50%;"} 
![](figures/fetal-ultrasound-image-synthesis/diffusion-based-generative-model.svg){fig-align="center" width="100%"}
:::

::: {style="font-size: 40%;"}
(a) Ho et al. 2020 ”Denoising Diffusion Probabilistic Models” https://arxiv.org/abs/2006.11239 
(b) Fiorentino et al. 2022 ”A Review on Deep Learning Algorithms for Fetal Ultrasound-Image Analysis” https://arxiv.org/abs/2201.12260
:::


## xfetus :baby: :brain: :robot:

A library for ultrasound fetal imaging synthesis using:  

* GANs,   
* transformers, and   
* diffusion models.  

::: {style="font-size: 50%;"}
{{< fa brands github >}}  [https://github.com/budai4medtech/xfetus](https://github.com/budai4medtech/xfetus)
:::

::: {.notes}
TODO:
* Resolve PRs https://github.com/budai4medtech/xfetus/pulls
* Show emojis in the main README 
:::




# Developing real-time AI applications for diagnosis

## :medical_symbol: Endoscopic Pituitary Surgery

{{< video https://www.youtube.com/embed/EwlRdxokdGk
    start="11"
    width="85%" 
    height="85%"
>}}

::: {.notes}
94,961 views  20 Nov 2012
Barrow Neurological Institute Neurosurgeon Andrew S. Little, MD, demonstrates the process of removing a tumor of the pituitary gland using minimally-invasive endoscopic neurosurgery.
https://www.youtube.com/watch?app=desktop&v=EwlRdxokdGk

553,519 views  28 May 2017
The pituitary gland is located at the bottom of your brain and above the inside of your nose. Endoscopic pituitary surgery (also called transsphenoidal endoscopic surgery) is a minimally invasive surgery performed through the nose and sphenoid sinus to remove pituitary tumors.
https://www.youtube.com/watch?v=lwmgNLwt_ts

Mao, Zhehua, Adrito Das, Mobarakol Islam, Danyal Z. Khan, Simon C. Williams, John G. Hanrahan, Anouk Borg et al. "PitSurgRT: real-time localization of critical anatomical structures in endoscopic pituitary surgery." International Journal of Computer Assisted Radiology and Surgery (2024): 1-8.
:::


## Real-time AI Applications for Surgery
::: {#fig-template}
::: { style="margin-top: 0px; font-size: 80%;"} 
![](figures/holoscan-platform/rtai4spipeline.svg){fig-align="center" width="100%"}
:::
Development and deployment pipeline for real-time AI apps for surgery
:::

::: {.notes}
Pipeline with development and deployment of real-time AI apps for surgery

{fig-align=center}
{fig-pos='b'}
b(bottom)
h(here)
p(page)
t(top)
:::



## NVIDIA Holoscan platform
:::: {.columns}

::: {.column width="50%"}
Holoscan-SDK

![](figures/holoscan-platform/holohub.svg)

[{{< fa brands github >}} `holoscan-sdk`](
https://github.com/nvidia-holoscan/holoscan-sdk/tree/main
)

[{{< fa brands github >}} `holohub`](
https://github.com/nvidia-holoscan/holohub
)

:::

::: {.column width="50%"}
Clara-AGX

![](figures/holoscan-platform/clara_agx_dev_kit_components.svg)

[{{< fa brands github >}} `Clara-AGX DevKit`](
https://github.com/nvidia-holoscan/holoscan-docs/blob/main/devkits/clara-agx/clara_agx_user_guide.md
)

[{{< fa brands github >}} `Orin-IGX DevKit`](
https://github.com/nvidia-holoscan/holoscan-docs/blob/main/devkits/nvidia-igx-orin/nvidia_igx_orin_user_guide.md
)

:::

::::

::: {.notes}
Holoscan platform
:::



## Holoscan Core Concepts
::: {#fig-template}

![](figures/holoscan-platform/holoscan_core_concepts.svg){fig-align=center}

Operator: An operator is the most basic unit of work in this framework.
:::

::: {.notes}

https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html
:::






## Bring Your Own Model (BYOM) {.scrollable}
::: {.panel-tabset}

### Workflow
::: {#fig-template}

![](figures/holoscan-platform/byom.svg){fig-align=center}

Connecting Operators
:::

### Python

```{.python}
import os
from argparse import ArgumentParser

from holoscan.core import Application

from holoscan.operators import (
    FormatConverterOp,
    HolovizOp,
    InferenceOp,
    SegmentationPostprocessorOp,
    VideoStreamReplayerOp,
)
from holoscan.resources import UnboundedAllocator


class BYOMApp(Application):
    def __init__(self, data):
        """Initialize the application

Parameters
----------
data : Location to the data
"""

        super().__init__()

        # set name
        self.name = "BYOM App"

        if data == "none":
            data = os.environ.get("HOLOSCAN_INPUT_PATH", "../data")

        self.sample_data_path = data

        self.model_path = os.path.join(os.path.dirname(__file__), "../model")
        self.model_path_map = {
            "byom_model": os.path.join(self.model_path, "identity_model.onnx"),
        }

        self.video_dir = os.path.join(self.sample_data_path, "racerx")
        if not os.path.exists(self.video_dir):
            raise ValueError(f"Could not find video data:{self.video_dir=}")

# Define the workflow
        self.add_flow(source, viz, {("output", "receivers")})
        self.add_flow(source, preprocessor, {("output", "source_video")})
        self.add_flow(preprocessor, inference, {("tensor", "receivers")})
        self.add_flow(inference, postprocessor, {("transmitter", "in_tensor")})
        self.add_flow(postprocessor, viz, {("out_tensor", "receivers")})


def main(config_file, data):
    app = BYOMApp(data=data)
    # if the --config command line argument was provided, it will override this config_file
    app.config(config_file)
    app.run()


if __name__ == "__main__":
    # Parse args
    parser = ArgumentParser(description="BYOM demo application.")
    parser.add_argument(
        "-d",
        "--data",
        default="none",
        help=("Set the data path"),
    )

    args = parser.parse_args()
    config_file = os.path.join(os.path.dirname(__file__), "byom.yaml")
    main(config_file=config_file, data=args.data)

```

### YAML

```{.python}
%YAML 1.2
replayer:  # VideoStreamReplayer
  basename: "racerx"
  frame_rate: 0 # as specified in timestamps
  repeat: true # default: false
  realtime: true # default: true
  count: 0 # default: 0 (no frame count restriction)

preprocessor:  # FormatConverter
  out_tensor_name: source_video
  out_dtype: "float32"
  resize_width: 512
  resize_height: 512

inference:  # Inference
  backend: "trt"
  pre_processor_map:
    "byom_model": ["source_video"]
  inference_map:
    "byom_model": ["output"]

postprocessor:  # SegmentationPostprocessor
  in_tensor_name: output
  # network_output_type: None
  data_format: nchw

viz:  # Holoviz
  width: 854
  height: 480
  color_lut: [
    [0.65, 0.81, 0.89, 0.1],
    ]
```

:::

::: {.notes}
Speaker notes go here.
:::



## {{< fa brands github >}} `real-time-ai-for-surgery`
### Getting started docs
::: {#fig-template}

![](figures/real-time-ai-for-surgery/getting-started.svg){fig-align=center}

Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow. 
:::

::: {.notes}
Speaker notes go here.
:::



## {{< fa brands github >}} `real-time-ai-for-surgery`
### 🏥 Endoscopic pituitary surgery
::: {.panel-tabset}


### 👃 Multi-head Model

![](figures/real-time-ai-for-surgery/eps-mhm.svg){fig-align=center}

### 🌓 PhaseNet Model

![](figures/real-time-ai-for-surgery/eps-pnm.svg){fig-align=center}

:::

::: {.notes}
Speaker notes go here.
:::




## {{< fa brands github >}} `real-time-ai-for-surgery` {.scrollable}
### 🏥 Endoscopic pituitary surgery
::: {.panel-tabset}

### 🔱 Multi AI models (python)

```{.python filename="multi-ai.py"}

...

        # Define the workflow
        if is_v4l2:
            self.add_flow(source, viz, {("signal", "receivers")})
            self.add_flow(source, preprocessor_v4l2, {("signal", "source_video")})
            self.add_flow(source, preprocessor_phasenet_v4l2, {("signal", "source_video")})
            for op in [preprocessor_v4l2, preprocessor_phasenet_v4l2]:
                self.add_flow(op, multi_ai_inference_v4l2, {("", "receivers")})
            ### connect infereceOp to postprocessors
            self.add_flow(
                multi_ai_inference_v4l2, multiheadOp, {("transmitter", "in_tensor_postproOp")}
            )
            self.add_flow(multi_ai_inference_v4l2, segpostprocessor, {("transmitter", "")})
            self.add_flow(multi_ai_inference_v4l2, phasenetOp, {("", "in")})

        else:
            self.add_flow(source, viz, {("", "receivers")})
            self.add_flow(source, preprocessor_replayer, {("output", "source_video")})
            self.add_flow(source, preprocessor_phasenet_replayer, {("output", "source_video")})
            for op in [preprocessor_replayer, preprocessor_phasenet_replayer]:
                self.add_flow(op, multi_ai_inference_replayer, {("", "receivers")})
            ### connect infereceOp to postprocessors
            self.add_flow(
                multi_ai_inference_replayer, multiheadOp, {("transmitter", "in_tensor_postproOp")}
            )
            self.add_flow(multi_ai_inference_replayer, segpostprocessor, {("transmitter", "")})
            self.add_flow(multi_ai_inference_replayer, phasenetOp, {("", "in")})

        ## connect postprocessors outputs for visualisation with holoviz
        self.add_flow(multiheadOp, viz, {("out_tensor_postproOp", "receivers")})
        self.add_flow(segpostprocessor, viz, {("", "receivers")})
        self.add_flow(phasenetOp, viz, {("out", "receivers")})
        self.add_flow(phasenetOp, viz, {("output_specs", "input_specs")})

...

```


### 🔱 Multi AI models (YAML)
```{.python filename="multi-ai.yaml"}

...

 multi_ai_inference_v4l2:
  #
  #
  # Multi-AI Inference Operator InferenceOp()
  #
  #
  backend: "trt"
  pre_processor_map:
    "pit_surg_model": ["prepro_v4l2"]
    "phasenet_model": ["prepro_PNv4l2"]
  inference_map:
    "pit_surg_model": ["segmentation_masks", "landmarks"]
    "phasenet_model": ["out"]
  enable_fp16: False
  parallel_inference: true # optional param, default to true
  infer_on_cpu: false # optional param, default to false
  input_on_cuda: true # optional param, default to true
  output_on_cuda: true # optional param, default to true
  transmit_on_cuda: true # optional param, default to true
  is_engine_path: false # optional param, default to false

multi_ai_inference_replayer:
  #
  #
  # Multi-AI Inference Operator InferenceOp()
  #
  #
  backend: "trt"
  pre_processor_map:
    "pit_surg_model": ["prepro_replayer"]
    "phasenet_model": ["prepro_PNreplayer"]
  inference_map:
    "pit_surg_model": ["segmentation_masks", "landmarks"]
    "phasenet_model": ["out"]
  enable_fp16: False
  parallel_inference: true # optional param, default to true
  infer_on_cpu: false # optional param, default to false
  input_on_cuda: true # optional param, default to true
  output_on_cuda: true # optional param, default to true
  transmit_on_cuda: true # optional param, default to true
  is_engine_path: false # optional param, default to false

...

```

:::

::: {.notes}
Speaker notes go here.

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}
```{.python filename="unit-test-example.py" code-line-numbers="|30-36"}
:::


## {{< fa brands github >}} `real-time-ai-for-surgery`
### 🤝 Contributing
::: {#fig-template}

![](figures/real-time-ai-for-surgery/contributing.svg){fig-align=center}

real-time-ai-for-surgery follows the Contributor Covenant Code of Conduct. Contributions, issues and feature requests are welcome. 
:::

::: {.notes}
Speaker notes go here.
:::


## {{< fa brands github >}} `real-time-ai-for-surgery` {.scrollable}
### GitHub templates 
::: {.panel-tabset}

### 🎒 new users
![](figures/real-time-ai-for-surgery/gh_templates_new_users.svg){fig-align=center}

### 🔩 new models
![](figures/real-time-ai-for-surgery/gh_templates_new_models.svg){fig-align=center}

### :recycle: PRs
![](figures/real-time-ai-for-surgery/gh_templates_PRs.svg){fig-align=center}


:::

::: {.notes}
Speaker notes go here.
{.scrollable}
:::





## {{< fa brands github >}} `real-time-ai-for-surgery` {.scrollable}
### Release version summaries
::: {.panel-tabset}

### v0.1.0
![](figures/real-time-ai-for-surgery/v0.1.0-rtai4s.svg){fig-align=center}

### v0.2.0
![](figures/real-time-ai-for-surgery/v0.2.0-rtai4s.svg){fig-align=center}

### v0.3.0
![](figures/real-time-ai-for-surgery/v0.3.0-rtai4s.svg){fig-align=center}

### v0.4.0
![](figures/real-time-ai-for-surgery/v0.4.0-rtai4s.svg){fig-align=center}

### v0.5.0
![](figures/real-time-ai-for-surgery/v0.5.0-rtai4s.svg){fig-align=center}

:::

::: {.notes}
Speaker notes go here.
{.scrollable}
:::






# Open-Source Software for Surgical Technologies

## {background-image="figures/oss4surgtech/oss4st-hsmr24-p00.svg"}



## Scripts {{< fa brands github >}} [`code.py`]()

::: {.panel-tabset}

### Tab A

Content for `Tab A`

### Tab B

Content for `Tab B`

:::


## :hospital: Case A

## :hospital: Case B

## :wrench: Hackathon

## :recycle: Model verification
1. AI
2. Code
3. ...


## :school_satchel: Education
* A
* B 



# Key takeaways

## :medical_symbol: Template for figures

::: {#sec-hp style="margin-top: 0px; font-size: 50%;"} 
![](figures/00_template-vector-images/drawing-v00.svg){fig-align="center" width="100%"}
:::

## 🙌 Acknowledgements

* Diego Kaski
  * UCL Queen Square Institute of Neurology
* Zhehua Mao, Sophia Bano and Matt Clarkson
  * Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS)
* Steve Thompson
  * Advanced Research Computing Centre (ARC)
* Mikael Brudfors and Nadim Daher
  * NVIDIA Healthcare AI
